<!doctype html>
<html
  lang="en-US" 
  
    data-theme-mode="auto"
  
>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8" />
<meta
  name="viewport"
  content="width=device-width, initial-scale=1, shrink-to-fit=no"
/>







  

<title>
  D1 | Muondingu
</title>
<meta
  name="description"
  content="lvl.99"
/>




  
<script src="https://www.googletagmanager.com/gtag/js?id=true"></script>
<script data-pjax>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'true');
</script>








<script>
  window.siteConfig = JSON.parse("{\"anchor_icon\":\"f0c9\",\"clipboard\":{\"copyright\":{\"content\":\"Copyright of this article： All articles on this blog, except those with a special declaration, are licensed under the BY-NC-SA license. Please indicate the source when reprinting!\",\"count\":50,\"enable\":false},\"fail\":\"Ehh!?? (ﾟ⊿ﾟ)ﾂ\",\"success\":\"Copied!(*^▽^*)\"},\"code_block\":{\"expand\":true},\"icon_font\":false,\"outdate\":{\"daysago\":180,\"enable\":false,\"message\":\"本文最后更新于 {time}，请注意文中内容可能已经发生变化。\"}}");
</script>












  
  
  
    
  

  
  
  
    
  

  
    

<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link
  rel="preload"
  as="style"
  href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7cNoto%20Serif%20SC:400,400italic,700,700italic%7c&amp;display=swap"
/>
<link
  rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7cNoto%20Serif%20SC:400,400italic,700,700italic%7c&amp;display=swap"
  media="print"
  onload="this.media='all'"
/>






  


  <link rel="stylesheet" href="https://npm.webcache.cn/@fortawesome/fontawesome-free@6.5.1/css/regular.min.css" />

  <link rel="stylesheet" href="https://npm.webcache.cn/@fortawesome/fontawesome-free@6.5.1/css/solid.min.css" />


  


  <link
    rel="preload"
    as="style"
    href="https://npm.webcache.cn/@fortawesome/fontawesome-free@6.5.1/css/brands.min.css"
    onload="this.onload=null;this.rel='stylesheet'"
  />

  <link
    rel="preload"
    as="style"
    href="https://npm.webcache.cn/@fortawesome/fontawesome-free@6.5.1/css/v5-font-face.min.css"
    onload="this.onload=null;this.rel='stylesheet'"
  />

  <link
    rel="preload"
    as="style"
    href="https://npm.webcache.cn/@fortawesome/fontawesome-free@6.5.1/css/v4-font-face.min.css"
    onload="this.onload=null;this.rel='stylesheet'"
  />





  






 <link rel="stylesheet" href="/css/loader.css" />




  <meta property="og:type" content="website" />
  <meta property="og:title" content="D1 | Muondingu" />
  <meta
    property="og:description"
    content="lvl.99"
  />
  <meta property="og:url" content="//localhost:1313/post/d1---foundational-large-language-models--text-generation/" />
  <meta
    property="og:site_name"
    content=""
  />
  <meta
    property="og:image"
    content="/"
  />
  <meta property="article:author" content="Muondingu" />
  <meta property="article:published_time" content="2024-11-16T00:00:00&#43;00:00" />
  <meta property="article:modified_time" content="2024-11-16T00:00:00&#43;00:00" />
  
    <meta property="article:tag" content="GenAIphilosophy" />
  
  
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:image" content="/" />
  
  
  
  
  




<link rel="shortcut icon" href="/favicon.ico">







 <link rel="stylesheet" href="/css/main.css" />





  <link
    rel="preload"
    as="style"
    href="https://npm.webcache.cn/photoswipe@5.4.4/dist/photoswipe.css"
    onload="this.onload=null;this.rel='stylesheet'"
  />






  <link
    rel="preload"
    as="style"
    href="https://npm.webcache.cn/katex@0.16.9/dist/katex.min.css"
    onload="this.onload=null;this.rel='stylesheet'"
  />








  

  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/pace-js@1.2.4/pace.min.js"
    
    
    
    
    integrity="sha384-k6YtvFUEIuEFBdrLKJ3YAUbBki333tj1CSUisai5Cswsg9wcLNaPzsTHDswp4Az8" crossorigin="anonymous"
  ></script>





  


  <link rel="stylesheet" href="https://npm.webcache.cn/@reimujs/aos@0.1.0/dist/aos.css" />








  <link rel="stylesheet" href="https://npm.webcache.cn/aplayer@1.10.1/dist/APlayer.min.css" />





  </head>
  <body>
    
  <div id='loader'>
    <div class="loading-left-bg loading-bg"></div>
    <div class="loading-right-bg loading-bg"></div>
    <div class="spinner-box">
      <div class="loading-taichi">
        
          <img src="/images/iX0X1V01.svg" alt="loading" />
        
      </div>
      <div class="loading-word"></div>
    </div>
  </div>
  </div>
  <script>
    var time = null;
    var startLoading = () => {
      time = Date.now();
      document.getElementById('loader').classList.remove("loading");
    }
    var endLoading = () => {
      if (!time) {
        document.body.style.overflow = 'auto';
        document.getElementById('loader').classList.add("loading");
      } else {
        if (Date.now() - time > 500) {
          time = null;
          document.body.style.overflow = 'auto';
          document.getElementById('loader').classList.add("loading");
        } else {
          setTimeout(endLoading, 500 - (Date.now() - time));
          time = null;
        }
      }
    }
    window.addEventListener('DOMContentLoaded', endLoading);
    document.getElementById('loader').addEventListener('click', endLoading);
  </script>


<div id="copy-tooltip" style="pointer-events: none; opacity: 0; transition: all 0.2s ease; position: fixed;top: 50%;left: 50%;z-index: 999;transform: translate(-50%, -50%);color: white;background: rgba(0, 0, 0, 0.5);padding: 10px 15px;border-radius: 10px;">
</div>




    <div id="container">
      <div id="wrap">
        
<div id="header-nav">
  <nav id="main-nav">
    
      <span class="main-nav-link-wrap">
        <div class='main-nav-icon icon '>
          
            &#xf0c9;
          
        </div>
        <a class="main-nav-link" href="//localhost:1313/">Home</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <div class='main-nav-icon icon '>
          
            &#xf252;
          
        </div>
        <a class="main-nav-link" href="//localhost:1313/archives">Archives</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <div class='main-nav-icon icon '>
          
            &#xf02e;
          
        </div>
        <a class="main-nav-link" href="//localhost:1313/resources">Resources</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <div class='main-nav-icon icon '>
          
            &#xf0f4;
          
        </div>
        <a class="main-nav-link" href="//localhost:1313/about">About</a>
      </span>
    
    <a id="main-nav-toggle" class="nav-icon"></a>
  </nav>
  <nav id="sub-nav">
    
    
      <a id="nav-search-btn" class="nav-icon popup-trigger" title="Search"></a>
    
  </nav>
</div>

<header id="header">
  
    <picture>
      
    </picture>
    
      <img fetchpriority="high" src="//localhost:1313/abstract-fantasy-landscape-with-color-year-purple-tones.webp" alt="D1">
    
  

  <div id="header-outer">
    <div id="header-title">
      
        
        
          
        
  
        
          <a href="//localhost:1313/" id="logo">
            <h1 data-aos="slide-up">D1</h1>
          </a>
        
      
  
      
        
        
        <h2 id="subtitle-wrap" data-aos="slide-down">
          
        </h2>
      
    </div>
  </div>
</header>





        <div id="content"
          class="sidebar-left"
            >
          <aside id="sidebar">
  
  
    <meting-js
      theme="var(--color-link)"
      id="9102128200"
      server="netease"
      type="playlist"
      
      fixed="false"
      autoplay="false"
      
      order="random"
      preload="true"
      volume="0.3"
      mutex="true"
      list-folded="true"
      lrc-type="0"
      >
    </meting-js>
  
  
  <div class="sidebar-wrapper wrap-sticky">
    <div
      class="sidebar-wrap"
      data-aos="fade-up"
    >
      
        <div class="sidebar-toc-sidebar">
          <div class="sidebar-toc">
  <h3 class="toc-title">Contents</h3>
  <div class="sidebar-toc-wrapper toc-div-class">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#why-language-models-are-important">Why language models are important</a></li>
    <li><a href="#large-language-models">Large language models</a></li>
    <li><a href="#transformer">Transformer</a>
      <ul>
        <li><a href="#input-preparation-and-embedding">Input preparation and embedding</a></li>
        <li><a href="#multi-head-attention">Multi-head attention</a></li>
        <li><a href="#multi-head-attention-power-in-diversity">Multi-head attention: power in diversity</a></li>
        <li><a href="#layer-normalization-and-residual-connections">Layer normalization and residual connections</a></li>
        <li><a href="#feedforward-layer">Feedforward layer</a></li>
        <li><a href="#encoder-and-decoder">Encoder and decoder</a></li>
        <li><a href="#training-transformers">Training Transformers</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>

<style>
.sidebar-toc-wrapper {
  max-height: calc(100vh - 200px);
  overflow-y: auto;
  overflow: -moz-scrollbars-none;
  -ms-overflow-style: none;
}

.sidebar-toc-wrapper::-webkit-scrollbar {
  display: none;
}

.sidebar-toc-wrapper ul {
  list-style: none;
  padding-left: 1em;
}

.sidebar-toc-wrapper li {
  margin: 0.4em 0;
}

.sidebar-toc-wrapper a {
  color: var(--color-text);
  text-decoration: none;
  font-size: 0.95em;
  transition: all 0.2s ease;
  padding: 2px 6px;
  border-radius: 3px;
}

.sidebar-toc-wrapper a:hover {
  color: var(--color-meta);
  background-color: var(--color-meta-shadow);
}

.sidebar-toc-wrapper a.active {
  color: var(--color-link);
  background-color: rgba(var(--color-link-rgb), 0.1);
}

.sidebar-toc-wrapper a.active:hover {
  color: var(--color-link-hover);
  background-color: rgba(var(--color-link-rgb), 0.8);
}
</style>

<script>
document.addEventListener('DOMContentLoaded', function() {
  const tocLinks = document.querySelectorAll('.sidebar-toc-wrapper a');
  const sections = [];
  let lastActiveLink = null;

  tocLinks.forEach(link => {
    const href = link.getAttribute('href');
    if (href && href.startsWith('#')) {
      const section = document.getElementById(href.substring(1));
      if (section) {
        sections.push({ element: section, link: link });
      }
    }
  });

  function updateActiveSection() {
    const scrollPosition = window.scrollY + 100;
    let currentSection = null;

    for (const section of sections) {
      if (section.element.offsetTop <= scrollPosition) {
        currentSection = section;
      } else {
        break;
      }
    }

    if (currentSection && lastActiveLink !== currentSection.link) {
      if (lastActiveLink) {
        lastActiveLink.classList.remove('active');
      }
      currentSection.link.classList.add('active');
      lastActiveLink = currentSection.link;
    }
  }

  let scrollTimeout;
  window.addEventListener('scroll', () => {
    if (scrollTimeout) {
      window.cancelAnimationFrame(scrollTimeout);
    }
    scrollTimeout = window.requestAnimationFrame(updateActiveSection);
  });

  updateActiveSection();
});
</script>
        </div>
        <div class="sidebar-common-sidebar hidden">
          


  
  <div class="sidebar-author-mini">
    <img
      data-src="/avatar/avatar.webp"
      data-sizes="auto"
      alt="Muondingu"
      class="lazyload"
    />
  </div>
  <div class="sidebar-menu-mini">
    
      <a
        href="//localhost:1313/"
        class="sidebar-menu-item"
        aria-label="Home"
      >
        <div class='sidebar-menu-icon icon '>
          
            &#xf0c9;
          
        </div>
      </a>
    
      <a
        href="//localhost:1313/archives"
        class="sidebar-menu-item"
        aria-label="Archives"
      >
        <div class='sidebar-menu-icon icon '>
          
            &#xf252;
          
        </div>
      </a>
    
      <a
        href="//localhost:1313/resources"
        class="sidebar-menu-item"
        aria-label="Resources"
      >
        <div class='sidebar-menu-icon icon '>
          
            &#xf02e;
          
        </div>
      </a>
    
      <a
        href="//localhost:1313/about"
        class="sidebar-menu-item"
        aria-label="About"
      >
        <div class='sidebar-menu-icon icon '>
          
            &#xf0f4;
          
        </div>
      </a>
    
  </div>

  <style>
  .sidebar-author-mini {
    display: flex;
    justify-content: center;
    margin: 0.8rem 0;
    padding: 0.3rem;
  }

  .sidebar-author-mini img {
    width: 60px;
    height: 60px;
    border-radius: 50%;
    object-fit: cover;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    transition: transform 0.3s ease;
    padding: 1px;
    background: var(--color-wrap);
  }

  .sidebar-author-mini img:hover {
    transform: scale(1.05);
  }

  .sidebar-menu-mini {
    display: flex;
    justify-content: center;
    gap: 1rem;
    flex-wrap: wrap;
    padding: 0.3rem;
    margin-bottom: 0.5rem;
  }

  .sidebar-menu-item {
    color: var(--color-text);
    text-decoration: none;
    transition: all 0.3s ease;
  }

  .sidebar-menu-item:hover {
    color: var(--color-link);
    transform: translateY(-2px);
  }

  .sidebar-menu-icon {
    font-size: 1.2rem;
  }
  </style>






          
            
  
  
  
  
  
    
  
    
  
    
  
    
      
    
  
    
      
    
  
    
  
    
  
    
  
    
  
    
      
    
  
    
  
    
  

  
    <div class="sidebar-related">
      <h3 class="sidebar-related-title">Related Posts</h3>
      <div class="sidebar-related-posts">
        
          
          
            
          
          <div class="sidebar-related-item" data-aos="fade-left">
            <a href="/post/anonymous/" class="sidebar-related-link" title="Anonymous and Safe Content Publishing">
              <div class="sidebar-related-cover">
                
                  
                  
                    <img
                      data-src="https://d-sketon.top/img/_backwebp/bg1.webp"
                      data-sizes="auto"
                      alt="Anonymous and Safe Content Publishing"
                      class="lazyload"
                    />
                  
                
              </div>
              <div class="sidebar-related-info">
                <div class="sidebar-related-title">Anonymous and Safe...</div>
                <div class="sidebar-related-date">2025-02-21</div>
              </div>
            </a>
          </div>
        
          
          
          <div class="sidebar-related-item" data-aos="fade-left">
            <a href="/post/readme/" class="sidebar-related-link" title="readme">
              <div class="sidebar-related-cover">
                
                  
                  
                    <img
                      data-src="https://d-sketon.top/img/_backwebp/bg1.webp"
                      data-sizes="auto"
                      alt="readme"
                      class="lazyload"
                    />
                  
                
              </div>
              <div class="sidebar-related-info">
                <div class="sidebar-related-title">readme</div>
                <div class="sidebar-related-date">2025-02-21</div>
              </div>
            </a>
          </div>
        
          
          
          <div class="sidebar-related-item" data-aos="fade-left">
            <a href="/post/placeholder-text/" class="sidebar-related-link" title="Placeholder Text">
              <div class="sidebar-related-cover">
                
                  
                  
                    <img
                      data-src="https://d-sketon.top/img/_backwebp/bg1.webp"
                      data-sizes="auto"
                      alt="Placeholder Text"
                      class="lazyload"
                    />
                  
                
              </div>
              <div class="sidebar-related-info">
                <div class="sidebar-related-title">Placeholder Text</div>
                <div class="sidebar-related-date">2019-03-09</div>
              </div>
            </a>
          </div>
        
      </div>
      <div class="related-tags">
        
          <a href="//localhost:1313/tags/genai" class="related-tag">
            GENAI
          </a>
        
          <a href="//localhost:1313/tags/philosophy" class="related-tag">
            PHILOSOPHY
          </a>
        
      </div>
    </div>

    <style>
    .sidebar-related {
      margin-top: 1rem;
      padding: 0.8rem;
      background: var(--color-wrap);
      border-radius: 10px;
      width: 100%;
      box-sizing: border-box;
    }

    .related-tags {
      display: flex;
      flex-wrap: wrap;
      gap: 0.4rem;
      margin-top: 1rem;
      padding-top: 0.8rem;
      border-top: 1px solid var(--color-meta-shadow);
    }

    .related-tag {
      font-size: 0.75rem;
      padding: 2px 6px;
      border-radius: 10px;
      background: rgba(var(--color-link-rgb), 0.1);
      color: var(--color-link);
      text-decoration: none;
      transition: all 0.3s ease;
      max-width: 100%;
      overflow: hidden;
      text-overflow: ellipsis;
      white-space: nowrap;
    }

    .related-tag:hover {
      background: rgba(var(--color-link-rgb), 0.2);
      transform: translateY(-1px);
    }

    .sidebar-related-title {
      font-size: 1rem;
      font-weight: 600;
      margin-bottom: 0.8rem;
      color: var(--color-text);
      padding-bottom: 0.5rem;
      border-bottom: 1px solid var(--color-meta-shadow);
    }

    .sidebar-related-posts {
      display: flex;
      flex-direction: column;
      gap: 0.6rem;
    }

    .sidebar-related-item {
      transition: all 0.3s ease;
    }

    .sidebar-related-item:hover {
      transform: translateX(5px);
    }

    .sidebar-related-link {
      text-decoration: none;
      display: flex;
      align-items: center;
      gap: 0.6rem;
      width: 100%;
    }

    .sidebar-related-cover {
      flex-shrink: 0;
      width: 50px;
      height: 50px;
      border-radius: 6px;
      overflow: hidden;
    }

    .sidebar-related-cover img {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }

    .sidebar-related-info {
      flex-grow: 1;
      min-width: 0;
    }

    .sidebar-related-info .sidebar-related-title {
      font-size: 0.85rem;
      margin: 0;
      padding: 0;
      border: none;
      color: var(--color-text);
      overflow: hidden;
      text-overflow: ellipsis;
      white-space: nowrap;
      width: 100%;
    }

    .sidebar-related-date {
      font-size: 0.75rem;
      color: var(--color-meta);
      margin-top: 0.2rem;
    }
    </style>
  
 
          
        </div>
      

      
        <div class="sidebar-btn-wrapper" style="position:static">
          <div class="sidebar-toc-btn current"></div>
          <div class="sidebar-common-btn"></div>
        </div>
      
    </div>
  </div>

  <div class="sidebar-widget">
    
  </div>
</aside>

<style>
.sidebar-common-sidebar {
  display: flex;
  flex-direction: column;
  gap: 1rem;
}

.sidebar-common-sidebar.hidden {
  display: none;
}
</style>

<script>
document.addEventListener('DOMContentLoaded', function() {
  const tocBtn = document.querySelector('.sidebar-toc-btn');
  const commonBtn = document.querySelector('.sidebar-common-btn');
  const tocSidebar = document.querySelector('.sidebar-toc-sidebar');
  const commonSidebar = document.querySelector('.sidebar-common-sidebar');

  if (tocBtn && commonBtn) {
    tocBtn.addEventListener('click', function() {
      tocBtn.classList.add('current');
      commonBtn.classList.remove('current');
      tocSidebar.classList.remove('hidden');
      commonSidebar.classList.add('hidden');
    });

    commonBtn.addEventListener('click', function() {
      commonBtn.classList.add('current');
      tocBtn.classList.remove('current');
      commonSidebar.classList.remove('hidden');
      tocSidebar.classList.add('hidden');
    });
  }
});
</script>


          <section id="main">
  <div class="reading-progress-bar"></div>

<style>
.reading-progress-bar {
  position: fixed;
  top: 0;
  left: 0;
  width: 0;
  height: 5px;
  background: var(--color-link);
  z-index: 9999;
  transition: width 0.2s ease;
}
</style>

<script>
document.addEventListener('DOMContentLoaded', function() {
  const progressBar = document.querySelector('.reading-progress-bar');
  
  function updateProgressBar() {
    const docElement = document.documentElement;
    const totalHeight = docElement.scrollHeight - docElement.clientHeight;
    const progress = (window.scrollY / totalHeight) * 100;
    progressBar.style.width = `${progress}%`;
  }

  window.addEventListener('scroll', updateProgressBar);
  window.addEventListener('resize', updateProgressBar);
});
</script> 
<article
  class="h-entry article"
  itemprop="blogPost"
  itemscope
  itemtype="https://schema.org/BlogPosting"
>
  <div
    class="article-inner"
    data-aos="fade-up"
  >
    <div class="article-meta">
      <div class="article-date">
  <a
    href="//localhost:1313/post/d1---foundational-large-language-models--text-generation/"
    class="article-date-link"
    data-aos="zoom-in"
  >
    <time datetime="2024-11-16 00:00:00 &#43;0000 UTC" itemprop="datePublished"
      >2024-11-16</time
    >
    <time style="display: none;" id="post-update-time"
      >2024-11-16</time
    >
  </a>
</div>

      <div class="article-category">
  
    <a
      class="article-category-link"
      href="//localhost:1313/categories/paradigm-shift"
      data-aos="zoom-in"
      >PARADIGM-SHIFT</a
    >
  
</div>

    </div>
    <div class="hr-line"></div>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
      
        <h1 id="contents">
<a class="header-anchor" href="#contents"></a>
Contents
</h1><h2 id="why-language-models-are-important">
<a class="header-anchor" href="#why-language-models-are-important"></a>
Why language models are important
</h2><blockquote>
<p>[!PDF|yellow] [[Barektain et al_Foundational Large Language Models &amp; Text Generation.pdf#page=7&amp;selection=20,15,30,29&amp;color=yellow|Barektain et al_Foundational Large Language Models &amp; Text Generation, p.7]]</p>
<blockquote>
<p>Although foundational LLMs trained in a variety of tasks on large amounts of data perform very well out of the box and display emergent behaviors (e.g. the ability to perform tasks they have not been directly trained for) they can also be adapted to solve specific tasks where performance out of the box is not at the level desired through a process known as fine-tuning. This requires significantly less data and computational resources than training an LLM from scratch.</p></blockquote></blockquote>
<p><strong>The big question is: how do these large language models work?</strong>
The next section explores the core building blocks of LLMs,</p>
<ul>
<li>focusing on transformer architectures and their evolution from the original ‘Attention is all you need’ to the latest models such as Gemini, Google’s most capable LLM.</li>
<li>We also cover training and fine-tuning techniques, as well as methods to improve the speed of response generation. The whitepaper concludes with a few examples of how language models are used in practice.</li>
</ul>
<h2 id="large-language-models">
<a class="header-anchor" href="#large-language-models"></a>
Large language models
</h2><p>A language model predicts the probability of a sequence of words.</p>
<blockquote>
<p>[!PDF|yellow] [[Barektain et al_Foundational Large Language Models &amp; Text Generation.pdf#page=8&amp;selection=10,49,18,21&amp;color=yellow|Barektain et al_Foundational Large Language Models &amp; Text Generation, p.8]]</p>
<blockquote>
<p>Commonly, when given a prefix of text, a language model assigns probabilities to subsequent words. For example, given the prefix “The most famous city in the US is…”, a language model might predict high probabilities to the words “New York” and “Los Angeles” and low probabilities to the words “laptop” or “apple”.</p></blockquote></blockquote>
<p>You can create a basic language model by storing an n-gram table,2 while modern language models are often based on neural models, such as <strong>transformers</strong>
Before the invention of transformers, <strong>recurrent neural networks (RNNSs)</strong> were the popular approach for modeling sequences.</p>
<ul>
<li>In particular, <strong>“long short-term memory” (LSTM)</strong> and <strong>“gated recurrent unit” (GRU)</strong> were common architectures.</li>
<li>RNNs process input and output sequences sequentially. They generate a sequence of hidden states based on the previous hidden state and the current input.</li>
<li>The sequential nature of RNNs makes them compute-intensive and hard to parallelize during training
<blockquote>
<p>[!PDF|yellow] [[Barektain et al_Foundational Large Language Models &amp; Text Generation.pdf#page=8&amp;selection=34,0,38,23&amp;color=yellow|Barektain et al_Foundational Large Language Models &amp; Text Generation, p.8]]</p></blockquote>
</li>
</ul>
<blockquote>
<blockquote>
<p>This area includes language problems such as machine translation, text classification, text summarization, and questionanswering, among others</p></blockquote></blockquote>
<p><strong>Transformers</strong>, on the other hand</p>
<ul>
<li>are a type of neural network that can process sequences of tokens in parallel thanks to the self-attention mechanism</li>
<li>This makes them significantly faster to train, and more powerful compared to RNNs for handling longterm dependencies in long sequence tasks</li>
<li>However, the cost of self-attention in the original transformers is quadratic in the context length which limits the size of the context, while RNNs have a theoretically infinite context length</li>
<li>Transformers have become the most popular approach for sequence modeling and transduction problems in recent years</li>
</ul>
<h2 id="transformer">
<a class="header-anchor" href="#transformer"></a>
Transformer
</h2><blockquote>
<p>[!PDF|yellow] [[Barektain et al_Foundational Large Language Models &amp; Text Generation.pdf#page=9&amp;selection=6,0,10,63&amp;color=yellow|Barektain et al_Foundational Large Language Models &amp; Text Generation, p.9]]</p>
<blockquote>
<p>The transformer architecture was developed at Google in 2017 for use in a translation model.</p></blockquote></blockquote>
<p>It’s a sequence-to-sequence model capable of converting sequences from one domain into sequences in another domain.</p>
<blockquote>
<p>For example, translating French sentences to English sentences.</p></blockquote>
<p>The original transformer architecture consists of two parts: an encoder and a decoder.</p>
<ul>
<li><em>The encoder</em> converts the input text (e.g., a French sentence) into a representation, which is then passed to the decoder.</li>
<li><em>The decoder</em> uses this representation to generate the output text (e.g., an English translation) autoregressively.</li>
</ul>
<p>Notably, the size of the output of the transformer encoder is linear in the size of its input.</p>
<p><mark class="hltr-green">The transformer consists of multiple layers. A layer in a neural network comprises a set of parameters that perform a specific transformation on the data. </mark>
![[Files/clipboard/Barektain et al_Foundational Large Language Models &amp; Text Generation 1.webp]]</p>
<p>[[Barektain et al_Foundational Large Language Models &amp; Text Generation.pdf#page=10&amp;rect=50,145,543,675|Barektain et al_Foundational Large Language Models &amp; Text Generation, p.10]]</p>
<h3 id="input-preparation-and-embedding">
<a class="header-anchor" href="#input-preparation-and-embedding"></a>
Input preparation and embedding
</h3><p>To prepare language inputs for transformers, we convert an input sequence into tokens and then into input embeddings.
At a high level, an input embedding is a high-dimensional vector that represents the meaning of each token in the sentence.
This embedding is then fed into the transformer for processing. Generating an input embedding involves the following steps:
![[Barektain et al_Foundational Large Language Models &amp; Text Generation.pdf#page=11&amp;rect=56,254,550,444&amp;color=yellow|Barektain et al_Foundational Large Language Models &amp; Text Generation, p.11]]</p>
<h3 id="multi-head-attention">
<a class="header-anchor" href="#multi-head-attention"></a>
Multi-head attention
</h3><p>After converting input tokens into embedding vectors, you feed these embeddings into the multi-head attention module (see Figure 1).
<strong>Self-attention</strong> is a crucial mechanism in transformers; it enables them to focus on specific parts of the input sequence relevant to the task at hand and to capture long-range dependencies within sequences more effectively than traditional RNNs</p>
<blockquote>
<p>“The tiger jumped out of a tree to get a drink because it was thirsty.”</p></blockquote>
<p><strong>Self-attention</strong> helps to determine relationships between different words and phrases in sentences. For example, in this sentence, “the tiger” and “it” are the same object, so we would expect these two words to be strongly connected. Self-attention achieves this through the following steps:</p>
<ol>
<li><em>Creating queries, keys, and values</em>: Each input embedding is multiplied by three learned weight matrices (Wq, Wk, Wv) to generate query (Q), key (K), and value (V) vectors. These are like specialized representations of each word.
• Query: The query vector helps the model ask, “Which other words in the sequence are relevant to me?”
• Key: The key vector is like a label that helps the model identify how a word <mark class="hltr-red">might be relevant</mark> to other words in the sequence.
• Value: The value vector holds the actual word content information.</li>
<li><em>Calculating scores</em>: Scores are calculated to determine <mark class="hltr-red">how much each word should ‘attend’</mark> to other words. This is done by taking the dot product of the query vector of one word with the key vectors of all the words in the sequence.</li>
<li><em>Normalization</em>: The scores are divided by the square root of the key vector dimension (dk) for stability, then passed through a softmax function to obtain attention weights. These weights indicate <mark class="hltr-red">how strongly each word is connected</mark> to the others.</li>
<li><em>Weighted values</em>: Each value vector is multiplied by its corresponding attention weight. The results are summed up, producing a context-aware representation for each word.</li>
</ol>
<p>![[Files/clipboard/Barektain et al_Foundational Large Language Models &amp; Text Generation.webp]]</p>
<p>[[Barektain et al_Foundational Large Language Models &amp; Text Generation.pdf#page=13&amp;rect=47,108,561,561|Barektain et al_Foundational Large Language Models &amp; Text Generation, p.13]]</p>
<p>In practice, these computations are performed at the same time, by stacking the query, key and value vectors for all the tokens into Q, K and V matrices and multiplying them together as shown in Figure 3</p>
<p>![[Barektain et al_Foundational Large Language Models &amp; Text Generation.pdf#page=14&amp;rect=48,405,575,603&amp;color=yellow|Barektain et al_Foundational Large Language Models &amp; Text Generation, p.14]]</p>
<h3 id="multi-head-attention-power-in-diversity">
<a class="header-anchor" href="#multi-head-attention-power-in-diversity"></a>
Multi-head attention: power in diversity
</h3><ul>
<li>Multi-head attention employs multiple sets of Q, K, V weight matrices.</li>
<li>These run in parallel, each ‘head’ potentially focusing on different aspects of the input relationships.</li>
<li>The outputs from each head are concatenated and linearly transformed, giving the model a richer representation of the input sequence.</li>
</ul>

        <link rel="stylesheet" href="/css/vendors/admonitions.css">
  <div class="admonition note">
    <div class="admonition-header">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path d="M0 64C0 28.7 28.7 0 64 0L224 0l0 128c0 17.7 14.3 32 32 32l128 0 0 125.7-86.8 86.8c-10.3 10.3-17.5 23.1-21 37.2l-18.7 74.9c-2.3 9.2-1.8 18.8 1.3 27.5L64 512c-35.3 0-64-28.7-64-64L0 64zm384 64l-128 0L256 0 384 128zM549.8 235.7l14.4 14.4c15.6 15.6 15.6 40.9 0 56.6l-29.4 29.4-71-71 29.4-29.4c15.6-15.6 40.9-15.6 56.6 0zM311.9 417L441.1 287.8l71 71L382.9 487.9c-4.1 4.1-9.2 7-14.9 8.4l-60.1 15c-5.5 1.4-11.2-.2-15.2-4.2s-5.6-9.7-4.2-15.2l15-60.1c1.4-5.6 4.3-10.8 8.4-14.9z"/></svg>
      <span>The use of multi-head attention improves the model’s ability to handle complex language patterns and long-range dependencies. The mechanism enables the transformer to consider multiple interpretations and representations of the input, which enhances its performance on these tasks.</span>
    </div>
  </div>
<h3 id="layer-normalization-and-residual-connections">
<a class="header-anchor" href="#layer-normalization-and-residual-connections"></a>
Layer normalization and residual connections
</h3><p>Each layer in a transformer, consisting of a multi-head attention module and <em>a feed-forward layer, employs layer normalization and residual connections</em>. This corresponds to the Add and Norm layer in [[Barektain et al_Foundational Large Language Models &amp; Text Generation.pdf#page=10&amp;rect=50,145,543,675|Fig1]]</p>
<ul>
<li>where ‘Add’ corresponds to the residual connection
<blockquote>
<p>Residual connections propagate the inputs to the output of one or more layers. This has the effect of making the optimization procedure easier to learn and also helps deal with vanishing and exploding gradients.</p></blockquote>
</li>
<li>and ‘Norm’ corresponds to layer normalization.
<blockquote>
<p>Layer normalization computes the <em>mean</em>( average value) and <em>variance</em>( how much the activations deviate from the mean) of the activations to normalize the activations in a given layer. This is typically performed to reduce covariate shift as well as improve gradient flow to yield faster convergence during training as well as improved overall performance.</p></blockquote>
</li>
</ul>
<h3 id="feedforward-layer">
<a class="header-anchor" href="#feedforward-layer"></a>
Feedforward layer
</h3><p>[[Barektain et al_Foundational Large Language Models &amp; Text Generation.pdf#page=15&amp;selection=38,0,56,19|p.15]]</p>
<ul>
<li><strong>Output of multi-head attention</strong> and <strong>&lsquo;Add and Norm&rsquo;</strong> is passed to the feedforward layer.</li>
<li><strong>Feedforward layer</strong> applies a <strong>position-wise transformation</strong> for each sequence position, adding <strong>non-linearity</strong> and <strong>complexity</strong>.</li>
<li>Structure: Two <strong>linear transformations</strong> with a <strong>non-linear activation</strong> (e.g., ReLU, GELU) in between. Enhances <strong>representational power</strong> of the model.</li>
<li>Processed data undergoes another <strong>&lsquo;Add and Norm&rsquo;</strong>, ensuring <strong>stability</strong> and <strong>effectiveness</strong> in deep transformer models.</li>
</ul>
<h3 id="encoder-and-decoder">
<a class="header-anchor" href="#encoder-and-decoder"></a>
Encoder and decoder
</h3><p>[[Barektain et al_Foundational Large Language Models &amp; Text Generation.pdf#page=16|p.16]]</p>
<ol>
<li>
<p>The original transformer architecture relies on a combination of encoder and decoder modules.
Each encoder and decoder consists of a series of layers, with each layer comprising key components:</p>
<ul>
<li>Multi-head self-attention</li>
<li>Position-wise feedforward network</li>
<li>Normalization layers</li>
<li>Residual connections</li>
</ul>
</li>
<li>
<p><strong>Encoder Functionality</strong>:</p>
<ul>
<li>Processes input sequence into a continuous representation that holds <strong>contextual representations</strong> for each token.</li>
<li>Workflow:
<ul>
<li>Normalize, tokenize, and convert input to embeddings.</li>
<li>Add <strong>positional encodings</strong> to retain sequence order.</li>
<li>Use <strong>self-attention</strong> for tokens to dynamically attend to one another, capturing <strong>contextual relationships</strong>.</li>
</ul>
</li>
<li>Output: Embedding vectors ( Z ) for the sequence.</li>
</ul>
</li>
<li>
<p><strong>Decoder Functionality</strong>:</p>
<ul>
<li>Generates the output sequence based on ( Z ).</li>
<li>Workflow:
<ul>
<li>Begins with a <strong>start-of-sequence token</strong>.</li>
<li><strong>Masked self-attention</strong>: Ensures tokens attend only to earlier positions (preserves <strong>auto-regressive property</strong>).</li>
<li><strong>Encoder-decoder cross-attention</strong>: Focuses on relevant parts of ( Z ).</li>
<li>Stops at an <strong>end-of-sequence token</strong>.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="decoder-only-transformer-used-in-llms">
<a class="header-anchor" href="#decoder-only-transformer-used-in-llms"></a>
<strong>Decoder-Only Transformer (Used in LLMs)</strong>:
</h4><ul>
<li><strong>Simplified Design</strong>: Eliminates encoder-decoder separation. Fcusing instead on directly generating the output sequence from the input.</li>
<li>Workflow:
<ul>
<li>Input undergoes a similar process of <strong>embedding and positional encoding</strong>.</li>
<li>Decoder uses <strong>masked self-attention</strong> to generate predictions for each subsequent token based on the previously generated tokens</li>
</ul>
</li>
<li><strong>Advantage</strong>: Streamlined for tasks where encoding and decoding can merge effectively.</li>
</ul>
<h3 id="training-transformers">
<a class="header-anchor" href="#training-transformers"></a>
Training Transformers
</h3><p><strong>Key Differences Between Training and Inference</strong>:</p>
<ul>
<li><strong>Training</strong>: Adjusts model parameters using <strong>loss functions</strong> and <strong>backpropagation</strong>.</li>
<li><strong>Inference</strong>: Uses fixed parameters to generate outputs without updating weights.</li>
</ul>
<hr>
<h4 id="data-preparation">
<a class="header-anchor" href="#data-preparation"></a>
Data preparation
</h4><p><strong>Steps in Training Transformers</strong>:</p>
<ol>
<li>
<p><strong>Data Preparation</strong>:</p>
<ul>
<li><strong>Clean the Data</strong>: Apply <strong>filtering</strong>, <strong>deduplication</strong>, and <strong>normalization</strong>.</li>
<li><strong>Tokenization</strong>: Convert text into tokens using methods like <strong>Byte-Pair Encoding</strong> or <strong>Unigram tokenization</strong>, creating a vocabulary for the model.</li>
<li><strong>Data Splitting</strong>: Divide into training and test datasets for model development and evaluation.</li>
</ul>
</li>
<li>
<p><strong>Training Process</strong>:</p>
<ul>
<li>Sample batches of <strong>input sequences</strong> and their <strong>corresponding targets</strong>.</li>
<li><strong>Input sequences</strong> are fed into the transformer, generating predicted outputs.</li>
<li>Compare predictions with targets using a <strong>loss function</strong> (e.g., <strong>cross-entropy loss</strong>) to compute gradients.</li>
<li>Use an <strong>optimizer</strong> (e.g., Adam) to update parameters based on gradients.</li>
<li>Repeat until the model reaches desired performance or processes a specific number of tokens.</li>
</ul>
</li>
</ol>
<hr>
<h4 id="training-tasks-by-model-type">
<a class="header-anchor" href="#training-tasks-by-model-type"></a>
<strong>Training Tasks by Model Type</strong>:
</h4><ol>
<li>
<p><strong>Decoder-Only Models</strong> (e.g., GPT):</p>
<ul>
<li>Task: <strong>Language modeling</strong></li>
<li>Target: Shifted version of the input sequence (e.g., &ldquo;The cat sat on&rdquo; → predict &ldquo;the&rdquo;).</li>
</ul>
</li>
<li>
<p><strong>Encoder-Only Models</strong> (e.g., BERT):</p>
<ul>
<li>Task: <strong>Masked language modeling (MLM)</strong></li>
<li>Example: &ldquo;The [MASK] sat on the mat&rdquo; → predict &ldquo;cat&rdquo;.</li>
</ul>
</li>
<li>
<p><strong>Encoder-Decoder Models</strong> (e.g., Original Transformer):</p>
<ul>
<li>Task: <strong>Sequence-to-sequence tasks</strong> (e.g., translation, summarization, question-answering).</li>
<li>Example: Translate &ldquo;Le chat est assis sur le tapis&rdquo; → &ldquo;The cat sat on the mat&rdquo;.</li>
</ul>
</li>
</ol>
<hr>
<h4 id="additional-training-considerations">
<a class="header-anchor" href="#additional-training-considerations"></a>
<strong>Additional Training Considerations</strong>:
</h4><ul>
<li><strong>Context Length</strong>:
<ul>
<li>Refers to how many previous tokens the model uses to predict the next one.</li>
<li><strong>Trade-offs</strong>:
<ul>
<li>Longer contexts improve model understanding of complex dependencies.</li>
<li>Increased computation and memory costs.</li>
</ul>
</li>
<li>Must balance context length with task complexity and resource limits.</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        






<blockquote class="article-copyright">
  
    <p><strong><span class="icon-user icon"></span> </strong>Muondingu @ </p>
  
  
  
    <p><strong><span class="icon-pencil icon"></span> </strong>D1</p>
  
  
    <p><strong><span class="icon-calendar icon"></span> </strong>2024-11-16 00:00:00</p>
  
  
  
</blockquote>

      

      

      
        

<div class="share-wrapper">
  
    <a href="https://www.facebook.com/sharer/sharer.php?u=//localhost:1313/post/d1---foundational-large-language-models--text-generation/" target="_blank" rel="noopener noreferrer" title="D1">
      <div class="share-icon icon icon-facebook">
        
      </div>
    </a>
  
    <a href="https://twitter.com/intent/tweet?url=//localhost:1313/post/d1---foundational-large-language-models--text-generation/&amp;text=D1&amp;via=//localhost:1313/" target="_blank" rel="noopener noreferrer" title="D1">
      <div class="share-icon icon icon-twitter">
        
      </div>
    </a>
  
</div>
      

      

      

      

      

      
      <ul class="article-tag-list" itemprop="keywords">
  
    <li class="article-tag-list-item" data-aos="zoom-in">
      <a
        class="article-tag-list-link"
        href="/tags/genai"
        rel="tag"
        >GENAI</a
      >
    </li>
  
    <li class="article-tag-list-item" data-aos="zoom-in">
      <a
        class="article-tag-list-link"
        href="/tags/philosophy"
        rel="tag"
        >PHILOSOPHY</a
      >
    </li>
  
</ul>

    </footer>
  </div>
  
    
  <nav
    id="article-nav"
    data-aos="fade-up"
  >
    
      <div class="article-nav-link-wrap article-nav-link-left">
        
          
          
            <img
              data-src="https://d-sketon.top/img/_backwebp/bg1.webp"
              data-sizes="auto"
              alt="Markdown Syntax Guide"
              class="lazyload"
            />
          
        
        <a href="//localhost:1313/post/markdown-syntax/"></a>
        <div class="article-nav-caption">Newer</div>
        <h3 class="article-nav-title">
          
            Markdown Syntax Guide
          
        </h3>
      </div>
    

    
      <div class="article-nav-link-wrap article-nav-link-right">
        
          
          
            <img
              data-src="https://d-sketon.top/img/_backwebp/bg1.webp"
              data-sizes="auto"
              alt="readme"
              class="lazyload"
            />
          
        
        <a href="//localhost:1313/post/readme/"></a>
        <div class="article-nav-caption">Older</div>
        <h3 class="article-nav-title">
          
            readme
          
        </h3>
      </div>
    
  </nav>




  
</article>










</section>
        </div>
        
        
        



  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  



<footer id="footer">
  <div style="width: 100%; overflow: hidden">
    <div class="footer-line"></div>
  </div>
  <div id="footer-info">
    <div>
      <span class="icon-copyright"></span>
       -
      2025
      <span class="footer-info-sep rotate"></span>
      Muondingu
    </div>
    
    
      <div>
        <span class="icon-brush"
          >&nbsp;
            11.0k
          </span
        >
        &nbsp;|&nbsp;
        <span class="icon-coffee">&nbsp;
          
          

          00:58
        </span>
      </div>
    
    
    
    
      <div>
        <span class="icon-eye"></span>
        <span id="busuanzi_container_site_pv"
          >Number of visits&nbsp;<span
            id="busuanzi_value_site_pv"
          ></span
        ></span>
        &nbsp;|&nbsp;
        <span class="icon-user"></span>
        <span id="busuanzi_container_site_uv"
          >Number of visitors&nbsp;<span
            id="busuanzi_value_site_uv"
          ></span
        ></span>
      </div>
    
  </div>
</footer>


        
          <div class="sidebar-top">
            <div class="sidebar-top-taichi rotate"></div>
            <div class="arrow-up"></div>
          </div>
        
        <div id="mask" class="hide"></div>
      </div>
      <nav id="mobile-nav">
  <div class="sidebar-wrap">
    
      <div class="sidebar-toc-sidebar">
        <div class="sidebar-toc">
  <h3 class="toc-title">Contents</h3>
  <div class="sidebar-toc-wrapper toc-div-class">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#why-language-models-are-important">Why language models are important</a></li>
    <li><a href="#large-language-models">Large language models</a></li>
    <li><a href="#transformer">Transformer</a>
      <ul>
        <li><a href="#input-preparation-and-embedding">Input preparation and embedding</a></li>
        <li><a href="#multi-head-attention">Multi-head attention</a></li>
        <li><a href="#multi-head-attention-power-in-diversity">Multi-head attention: power in diversity</a></li>
        <li><a href="#layer-normalization-and-residual-connections">Layer normalization and residual connections</a></li>
        <li><a href="#feedforward-layer">Feedforward layer</a></li>
        <li><a href="#encoder-and-decoder">Encoder and decoder</a></li>
        <li><a href="#training-transformers">Training Transformers</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>

<style>
.sidebar-toc-wrapper {
  max-height: calc(100vh - 200px);
  overflow-y: auto;
  overflow: -moz-scrollbars-none;
  -ms-overflow-style: none;
}

.sidebar-toc-wrapper::-webkit-scrollbar {
  display: none;
}

.sidebar-toc-wrapper ul {
  list-style: none;
  padding-left: 1em;
}

.sidebar-toc-wrapper li {
  margin: 0.4em 0;
}

.sidebar-toc-wrapper a {
  color: var(--color-text);
  text-decoration: none;
  font-size: 0.95em;
  transition: all 0.2s ease;
  padding: 2px 6px;
  border-radius: 3px;
}

.sidebar-toc-wrapper a:hover {
  color: var(--color-meta);
  background-color: var(--color-meta-shadow);
}

.sidebar-toc-wrapper a.active {
  color: var(--color-link);
  background-color: rgba(var(--color-link-rgb), 0.1);
}

.sidebar-toc-wrapper a.active:hover {
  color: var(--color-link-hover);
  background-color: rgba(var(--color-link-rgb), 0.8);
}
</style>

<script>
document.addEventListener('DOMContentLoaded', function() {
  const tocLinks = document.querySelectorAll('.sidebar-toc-wrapper a');
  const sections = [];
  let lastActiveLink = null;

  tocLinks.forEach(link => {
    const href = link.getAttribute('href');
    if (href && href.startsWith('#')) {
      const section = document.getElementById(href.substring(1));
      if (section) {
        sections.push({ element: section, link: link });
      }
    }
  });

  function updateActiveSection() {
    const scrollPosition = window.scrollY + 100;
    let currentSection = null;

    for (const section of sections) {
      if (section.element.offsetTop <= scrollPosition) {
        currentSection = section;
      } else {
        break;
      }
    }

    if (currentSection && lastActiveLink !== currentSection.link) {
      if (lastActiveLink) {
        lastActiveLink.classList.remove('active');
      }
      currentSection.link.classList.add('active');
      lastActiveLink = currentSection.link;
    }
  }

  let scrollTimeout;
  window.addEventListener('scroll', () => {
    if (scrollTimeout) {
      window.cancelAnimationFrame(scrollTimeout);
    }
    scrollTimeout = window.requestAnimationFrame(updateActiveSection);
  });

  updateActiveSection();
});
</script>
      </div>
      <div class="sidebar-common-sidebar hidden">
        


  
  <div class="sidebar-author-mini">
    <img
      data-src="/avatar/avatar.webp"
      data-sizes="auto"
      alt="Muondingu"
      class="lazyload"
    />
  </div>
  <div class="sidebar-menu-mini">
    
      <a
        href="//localhost:1313/"
        class="sidebar-menu-item"
        aria-label="Home"
      >
        <div class='sidebar-menu-icon icon '>
          
            &#xf0c9;
          
        </div>
      </a>
    
      <a
        href="//localhost:1313/archives"
        class="sidebar-menu-item"
        aria-label="Archives"
      >
        <div class='sidebar-menu-icon icon '>
          
            &#xf252;
          
        </div>
      </a>
    
      <a
        href="//localhost:1313/resources"
        class="sidebar-menu-item"
        aria-label="Resources"
      >
        <div class='sidebar-menu-icon icon '>
          
            &#xf02e;
          
        </div>
      </a>
    
      <a
        href="//localhost:1313/about"
        class="sidebar-menu-item"
        aria-label="About"
      >
        <div class='sidebar-menu-icon icon '>
          
            &#xf0f4;
          
        </div>
      </a>
    
  </div>

  <style>
  .sidebar-author-mini {
    display: flex;
    justify-content: center;
    margin: 0.8rem 0;
    padding: 0.3rem;
  }

  .sidebar-author-mini img {
    width: 60px;
    height: 60px;
    border-radius: 50%;
    object-fit: cover;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    transition: transform 0.3s ease;
    padding: 1px;
    background: var(--color-wrap);
  }

  .sidebar-author-mini img:hover {
    transform: scale(1.05);
  }

  .sidebar-menu-mini {
    display: flex;
    justify-content: center;
    gap: 1rem;
    flex-wrap: wrap;
    padding: 0.3rem;
    margin-bottom: 0.5rem;
  }

  .sidebar-menu-item {
    color: var(--color-text);
    text-decoration: none;
    transition: all 0.3s ease;
  }

  .sidebar-menu-item:hover {
    color: var(--color-link);
    transform: translateY(-2px);
  }

  .sidebar-menu-icon {
    font-size: 1.2rem;
  }
  </style>






      </div>
    
  </div>
  
    <div class="sidebar-btn-wrapper">
      <div class="sidebar-toc-btn current"></div>
      <div class="sidebar-common-btn"></div>
    </div>
  
</nav>


    </div>
    
      <div class="site-search">
        <div class="reimu-popup popup">
          <div class="reimu-search">
            <div class="reimu-search-input-icon"></div>
            <div class="reimu-search-input" id="reimu-search-input"></div>
            <div class="popup-btn-close"></div>
          </div>
          <div class="reimu-results">
            <div id="reimu-stats"></div>
            <div id="reimu-hits"></div>
            <div id="reimu-pagination" class="reimu-pagination"></div>
          </div>
          <img class="reimu-bg" src="/images/reimu.png" />
        </div>
      </div>
    
    






  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/lazysizes@5.3.2/lazysizes.min.js"
    
    
    
    
    integrity="sha384-3gT/vsepWkfz/ff7PpWNUeMzeWoH3cDhm/A8jM7ouoAK0/fP/9bcHHR5kHq2nf&#43;e" crossorigin="anonymous"
  ></script>




  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/clipboard@2.0.11/dist/clipboard.min.js"
    
    
    
    
    integrity="sha384-J08i8An/QeARD9ExYpvphB8BsyOj3Gh2TSh1aLINKO3L0cMSH2dN3E22zFoXEi0Q" crossorigin="anonymous"
  ></script>









  
      
      <script src="/js/main.js" ></script>
      



  





  
      
      <script src="/js/aos.js" ></script>
      

  <script>
    var aosInit = () => {
      AOS.init({
        duration: 1000,
        easing: "ease",
        once: true,
        offset: 50,
      });
    };
    if (document.readyState === "loading") {
      document.addEventListener("DOMContentLoaded", aosInit);
    } else {
      aosInit();
    }
  </script>








  
      
      <script src="/js/pjax_main.js" data-pjax></script>
      



  <script>
    var ALGOLIA_CONFIG = {
      logo: '\/images\/algolia_logo.svg',
      algolia: {
        applicationID: "78HHSJWHHA",
        apiKey: "db19c789a4fd2bdb15f43823fc74756f",
        indexName: "algolia",
        hits: {
          "per_page": parseInt("10")
        },
        labels: {
          "input_placeholder": "Search.....",
          "hits_empty": "No content found related to 「${query}",
          "hits_stats": "${hits} result found（took ${time} ms）"
        }
      }
    };
  </script>
  

  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/algoliasearch@4.17.1/dist/algoliasearch-lite.umd.js"
    defer
    
    
    
    integrity="sha384-xvLS0jfKuoREs7pqkRI/OI8GcqohO5S&#43;jQz7ZBtQXnsXmD&#43;9jDOOY4cL6dCPzlrk" crossorigin="anonymous"
  ></script>


  

  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/instantsearch.js@4.56.1/dist/instantsearch.production.min.js"
    defer
    
    
    
    integrity="sha384-hHJCflT4KBLQyHfKO9vpstIcFKn/Y&#43;KHTORelMMEn7mOp2AVPp&#43;7fr03dLgZiV3J" crossorigin="anonymous"
  ></script>


  





  
      
      <script src="/js/algolia_search.js" ></script>
      




  

  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/mouse-firework@0.1.0/dist/index.umd.js"
    
    
    
    
    integrity="sha384-KM6i7tu43nYd6e0beIljxHMC5tZc58XBDu7pPA58w50h18Jsx7gLdimfS09RXlKv" crossorigin="anonymous"
  ></script>


<script>
  if (window.firework) {
    const options = JSON.parse("{\"excludeelements\":[\"a\",\"button\"],\"particles\":[{\"colors\":[\"#ff5252\",\"#ff7c7c\",\"#ffafaf\",\"#ffd0d0\"],\"duration\":[1200,1800],\"easing\":\"easeOutExpo\",\"move\":[\"emit\"],\"number\":20,\"shape\":\"circle\",\"shapeOptions\":{\"alpha\":[0.3,0.5],\"radius\":[16,32]}},{\"colors\":[\"#ff0000\"],\"duration\":[1200,1800],\"easing\":\"easeOutExpo\",\"move\":[\"diffuse\"],\"number\":1,\"shape\":\"circle\",\"shapeOptions\":{\"alpha\":[0.2,0.5],\"lineWidth\":6,\"radius\":20}}]}");
    options.excludeElements = options.excludeelements;
    delete options.excludeelements;
    window.firework(options);
  }
</script>





<script>
  function initLive2d() {
    live2d.init('https:\/\/fastly.jsdelivr.net\/gh\/D-Sketon\/plugin-live2d\/', {themeTipsPath: ""});
  }
</script> 

  
  
  
  
  
  
  <script
    src="https://fastly.jsdelivr.net/gh/D-Sketon/plugin-live2d/js/live2d-autoload.js"
    
    async
    
    onload="initLive2d()"
    
  ></script>





  

  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/quicklink@2.3.0/dist/quicklink.umd.js"
    
    
    
    
    integrity="sha384-aD7FsuQkS1ohgFKY41fJfeA&#43;Wd/QRNnrOd9Bs58K3FzKdJJv8yPnYU8Tnp5z1agS" crossorigin="anonymous"
  ></script>


  <script data-pjax>
    window.quicklink?.listen({
      timeout:  3000 ,
      priority:  true ,
      ignores: JSON.parse("[]")
    });
  </script>


<div id="lazy-script">
  <div>
    
      
      
        
      
      <script data-pjax>
        window.REIMU_POST = {
          author: "Muondingu",
          title: "D1",
          url: "\/\/localhost:1313\/post\/d1---foundational-large-language-models--text-generation\/",
          description: "\rContents\rWhy language models are important\r[!PDF|yellow] [[Barektain et al_Foundational Large Language Models \u0026amp; Text Generation.pdf#page=7\u0026amp;selection=20,15,30,29\u0026amp;color=yellow|Barektain et al_Foundational Large Language Models \u0026amp; Text Generation, p.7]]\nAlthough foundational LLMs …",
          cover: "\/\/localhost:1313\/abstract-fantasy-landscape-with-color-year-purple-tones.webp",
        };
      </script>
    
    
    
      





  
      
      <script src="/js/insert_highlight.js" data-pjax></script>
      

      
      
      
      
      <script type="module" data-pjax>
        const PhotoSwipeLightbox = (await safeImport("https:\/\/npm.webcache.cn\/photoswipe@5.4.4\/dist\/photoswipe-lightbox.esm.min.js", "sha384-DiL6M\/gG\u002bwmTxmCRZyD1zee6lIhawn5TGvED0FOh7fXcN9B0aZ9dexSF\/N6lrZi\/")).default;

        const pswp = () => {
          if (_$$('.article-entry a.article-gallery-item').length > 0) {
            new PhotoSwipeLightbox({
              gallery: '.article-entry',
              children: 'a.article-gallery-item',
              pswpModule: () => safeImport("https:\/\/npm.webcache.cn\/photoswipe@5.4.4\/dist\/photoswipe.esm.min.js", "sha384-WkkO3GCmgkC3VQWpaV8DqhKJqpzpF9JoByxDmnV8\u002boTJ7m3DfYEWX1fu1scuS4\u002bs")
            }).init();
          }
          if(_$$('.article-gallery a.article-gallery-item').length > 0) {
            new PhotoSwipeLightbox({
              gallery: '.article-gallery',
              children: 'a.article-gallery-item',
              pswpModule: () => safeImport("https:\/\/npm.webcache.cn\/photoswipe@5.4.4\/dist\/photoswipe.esm.min.js", "sha384-WkkO3GCmgkC3VQWpaV8DqhKJqpzpF9JoByxDmnV8\u002boTJ7m3DfYEWX1fu1scuS4\u002bs")
            }).init();
          }
          window.lightboxStatus = 'done';
          window.removeEventListener('lightbox:ready', pswp);
        }
        if(window.lightboxStatus === 'ready') {
          pswp()
        } else {
          window.addEventListener('lightbox:ready', pswp);
        }
      </script>
      












      
    
    
  </div>
</div>




  

  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/busuanzi@2.3.0/bsz.pure.mini.js"
    
    async
    
    
    integrity="sha384-0M75wtSkhjIInv4coYlaJU83&#43;OypaRCIq2SukQVQX04eGTCBXJDuWAbJet56id&#43;S" crossorigin="anonymous"
  ></script>





  <script>
    if ('serviceWorker' in navigator) {
      navigator.serviceWorker.getRegistrations().then((registrations) => {
        for (let registration of registrations) {
          registration.unregister();
        }
      });
    }
  </script>


<script>
  const reimuCopyright = String.raw`
   ______     ______     __     __    __     __  __    
  /\  == \   /\  ___\   /\ \   /\ "-./  \   /\ \/\ \   
  \ \  __<   \ \  __\   \ \ \  \ \ \-./\ \  \ \ \_\ \  
   \ \_\ \_\  \ \_____\  \ \_\  \ \_\ \ \_\  \ \_____\ 
    \/_/ /_/   \/_____/   \/_/   \/_/  \/_/   \/_____/ 
                                                    
  `;
  console.log(String.raw`%c ${reimuCopyright}`, "color: #ff5252;");
  console.log(
    "%c Theme.Reimu" + " %c https://github.com/D-Sketon/hugo-theme-reimu ",
    "color: white; background: #ff5252; padding:5px 0;",
    "padding:4px;border:1px solid #ff5252;",
  );
</script>


  

  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/aplayer@1.10.1/dist/APlayer.min.js"
    
    
    
    
    integrity="sha384-gdGYZwHnfJM54evoZhpO0s6ZF5BQiybkiyW7VXr&#43;h5UfruuRL/aORyw&#43;5&#43;HZoU6e" crossorigin="anonymous"
  ></script>


  



  

  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/meting@2.0.1/dist/Meting.min.js"
    
    
    
    
    integrity="sha384-ASVlpKF80A22OXTK3tfEjZm1EL6uFMKIC4p8&#43;0maanw1S/IyB&#43;Y4JG&#43;ZDU7GpKE8" crossorigin="anonymous"
  ></script>


  

    

<style>
.lang-switcher {
  position: fixed;
  bottom: 20px;
  right: 40px;
  z-index: 1000;
}

.lang-main, .lang-option {
  width: 40px;
  height: 40px;
  border-radius: 50%;
  background-size: cover;
  background-position: center;
  border: 2px solid white;
  box-shadow: 0 2px 5px rgba(0,0,0,0.2);
  cursor: pointer;
  transition: all 0.3s ease;
}

.lang-alternatives {
  position: absolute;
  bottom: 0;
  right: 0;
  opacity: 0;
  visibility: hidden;
  transform: scale(0.5);
  transition: all 0.3s ease;
}

.lang-switcher:hover .lang-alternatives {
  opacity: 1;
  visibility: visible;
  transform: scale(1);
}

.lang-option {
  position: absolute;
  transition: all 0.3s ease;
}

.lang-switcher:hover .lang-option {
  transform: translateY(-55px) rotate(0deg);
}

.lang-switcher:hover .lang-option:nth-child(1) { transform: translate(-50px, -35px); }
.lang-switcher:hover .lang-option:nth-child(2) { transform: translate(-35px, -50px); }
.lang-switcher:hover .lang-option:nth-child(3) { transform: translate(-15px, -60px); }

.lang-main:hover {
  transform: scale(1.1);
  box-shadow: 0 4px 8px rgba(0,0,0,0.3);
}

 
.visually-hidden {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
</style>
  </body>
</html>



<script src="https://static.elfsight.com/platform/platform.js" async></script>
<div class="elfsight-app-4aad71cb-9178-4154-afbc-2b2b9c5658bc" data-elfsight-app-lazy></div>


<script src='https://storage.ko-fi.com/cdn/scripts/overlay-widget.js'></script>
<script>
  kofiWidgetOverlay.draw('muondingu', {
    'type': 'floating-chat',
    'floating-chat.donateButton.text': 'Meow!',
    'floating-chat.donateButton.background-color': '#f45d22',
    'floating-chat.donateButton.text-color': '#fff'
  });
</script>

<style>
.floating-chat-kofi-popup-iframe { 
  left: unset; 
  right: 126px; 
  bottom: 76px;
  z-index: 999999;  
}

.floatingchat-container-wrap { 
  left: unset; 
  right: 86px; 
  bottom: 20px;
  transition: transform 0.3s ease;
  z-index: 999998;  
}

.floatingchat-container-wrap:hover {
  transform: scale(1.05);
}

 
@media (max-width: 768px) {
  .floating-chat-kofi-popup-iframe {
    left: unset;
    right: 20px !important;
    bottom: unset;
    top: 20px !important;
    max-width: 90vw;  
  }

  .floatingchat-container-wrap {
    left: unset; 
    right: 20px !important;
    bottom: unset;
    top: 20px !important;
    max-width: 90vw;  
  }
}
</style>