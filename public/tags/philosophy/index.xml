<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Philosophy on Optimize. Automate. Gamify.</title>
    <link>//localhost:1313/tags/philosophy/</link>
    <description>Recent content in Philosophy on Optimize. Automate. Gamify.</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 23 Feb 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:1313/tags/philosophy/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>D2</title>
      <link>//localhost:1313/post/d1---foundational-large-language-models--text-generation/</link>
      <pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/post/d1---foundational-large-language-models--text-generation/</guid>
      <description>&lt;h1 id=&#34;contents&#34;&gt;&#xD;&#xA;&lt;a class=&#34;header-anchor&#34; href=&#34;#contents&#34;&gt;&lt;/a&gt;&#xD;&#xA;Contents&#xD;&#xA;&lt;/h1&gt;&lt;h2 id=&#34;why-language-models-are-important&#34;&gt;&#xD;&#xA;&lt;a class=&#34;header-anchor&#34; href=&#34;#why-language-models-are-important&#34;&gt;&lt;/a&gt;&#xD;&#xA;Why language models are important&#xD;&#xA;&lt;/h2&gt;&lt;blockquote&gt;&#xA;&lt;p&gt;[!PDF|yellow] [[Barektain et al_Foundational Large Language Models &amp;amp; Text Generation.pdf#page=7&amp;amp;selection=20,15,30,29&amp;amp;color=yellow|Barektain et al_Foundational Large Language Models &amp;amp; Text Generation, p.7]]&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Although foundational LLMs trained in a variety of tasks on large amounts of data perform very well out of the box and display emergent behaviors (e.g. the ability to perform tasks they have not been directly trained for) they can also be adapted to solve specific tasks where performance out of the box is not at the level desired through a process known as fine-tuning. This requires significantly less data and computational resources than training an LLM from scratch.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Anonymous and Safe Content Publishing</title>
      <link>//localhost:1313/post/anonymous/</link>
      <pubDate>Fri, 21 Feb 2025 19:17:31 +0700</pubDate>
      <guid>//localhost:1313/post/anonymous/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;//localhost:1313/images/gojocat.png&#34; alt=&#34;test&#34;&gt;&#xA;&lt;img src=&#34;//localhost:1313/post/anonymous/test2.jpg&#34; alt=&#34;test2&#34;&gt;&#xA;&lt;img src=&#34;//localhost:1313/1358788.png&#34; alt=&#34;test3&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;key-points-for-anonymous-and-safe-content-publishing&#34;&gt;&#xD;&#xA;&lt;a class=&#34;header-anchor&#34; href=&#34;#key-points-for-anonymous-and-safe-content-publishing&#34;&gt;&lt;/a&gt;&#xD;&#xA;Key Points for Anonymous and Safe Content Publishing&#xD;&#xA;&lt;/h1&gt;&lt;ul&gt;&#xA;&lt;li&gt;Use pseudonyms, anonymous emails (e.g., ProtonMail), VPNs (e.g., ExpressVPN), Tor browser, and remove metadata for technical anonymity.&lt;/li&gt;&#xA;&lt;li&gt;Choose platforms like self-hosted WordPress for better privacy; social media is riskier due to data collection.&lt;/li&gt;&#xA;&lt;li&gt;Write neutrally, avoid personal details, and assess content risks.&lt;/li&gt;&#xA;&lt;li&gt;Secure accounts with strong passwords, private browsing, and separate online/offline identities.&lt;/li&gt;&#xA;&lt;li&gt;Legally, avoid defamation and hate speech; use anonymity responsibly.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;technical-measures&#34;&gt;&#xD;&#xA;&lt;a class=&#34;header-anchor&#34; href=&#34;#technical-measures&#34;&gt;&lt;/a&gt;&#xD;&#xA;Technical Measures&#xD;&#xA;&lt;/h1&gt;&lt;p&gt;To publish anonymously, start with technical tools:&lt;/p&gt;</description>
    </item>
    <item>
      <title>readme</title>
      <link>//localhost:1313/post/readme/</link>
      <pubDate>Fri, 21 Feb 2025 19:17:31 +0700</pubDate>
      <guid>//localhost:1313/post/readme/</guid>
      <description>&lt;img src=&#34;https://cdn.jsdelivr.net/gh/D-Sketon/hexo-theme-reimu@main/_screenshot/Reimu_dark.png&#34;/&gt;&#xD;&#xA;&lt;div align = center&gt;&#xD;&#xA;  &lt;h1&gt;hexo-theme-reimu&lt;/h1&gt;&#xD;&#xA;  &lt;img alt=&#34;NPM License&#34; src=&#34;https://img.shields.io/npm/l/hexo-theme-reimu&#34;&gt;&#xD;&#xA;  &lt;img alt=&#34;NPM Version&#34; src=&#34;https://img.shields.io/npm/v/hexo-theme-reimu&#34;&gt;&#xD;&#xA;  &lt;img alt=&#34;GitHub Repo stars&#34; src=&#34;https://img.shields.io/github/stars/D-Sketon/hexo-theme-reimu&#34;&gt;&#xD;&#xA;  &lt;img src=&#34;https://wakatime.com/badge/user/a6ea8444-9e83-48bb-9744-09a19ac07114/project/fe59c195-6633-4ee8-89c0-e1b24fa1fff4.svg&#34; alt=&#34;wakatime&#34;&gt;&#xD;&#xA;  &lt;p align=&#34;center&#34;&gt;&#xD;&#xA;  ðŸ’˜ Hakurei Reimu ðŸ’˜&#xD;&#xA;  &lt;/p&gt;&#xD;&#xA;&lt;p&gt;&lt;a href=&#34;https://d-sketon.github.io&#34;&gt;Demo&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/D-Sketon/hexo-theme-reimu/blob/main/README.md&#34;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | English&lt;/p&gt;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;hr&gt;&#xA;&#xD;&#xA;        &lt;link rel=&#34;stylesheet&#34; href=&#34;//localhost:1313/css/vendors/admonitions.css&#34;&gt;&#xD;&#xA;  &lt;div class=&#34;admonition warning&#34;&gt;&#xD;&#xA;    &lt;div class=&#34;admonition-header&#34;&gt;&#xD;&#xA;      &lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 512 512&#34;&gt;&lt;path d=&#34;M256 32c14.2 0 27.3 7.5 34.5 19.8l216 368c7.3 12.4 7.3 27.7 .2 40.1S486.3 480 472 480L40 480c-14.3 0-27.6-7.7-34.7-20.1s-7-27.8 .2-40.1l216-368C228.7 39.5 241.8 32 256 32zm0 128c-13.3 0-24 10.7-24 24l0 112c0 13.3 10.7 24 24 24s24-10.7 24-24l0-112c0-13.3-10.7-24-24-24zm32 224a32 32 0 1 0 -64 0 32 32 0 1 0 64 0z&#34;/&gt;&lt;/svg&gt;&#xD;&#xA;      &lt;span&gt;Warning&lt;/span&gt;&#xD;&#xA;    &lt;/div&gt;&#xD;&#xA;      &lt;div class=&#34;admonition-content&#34;&gt;&#xD;&#xA;        &lt;p&gt;Versions below v1.0.0 have been deprecated. Please upgrade to version v1.0.0 or above as soon as possible.&lt;/p&gt;</description>
    </item>
    <item>
      <title>D1</title>
      <link>//localhost:1313/post/d1---foundational-large-language-models--text-generation---copy/</link>
      <pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/post/d1---foundational-large-language-models--text-generation---copy/</guid>
      <description>&lt;h1 id=&#34;contents&#34;&gt;&#xD;&#xA;&lt;a class=&#34;header-anchor&#34; href=&#34;#contents&#34;&gt;&lt;/a&gt;&#xD;&#xA;Contents&#xD;&#xA;&lt;/h1&gt;&lt;h2 id=&#34;why-language-models-are-important&#34;&gt;&#xD;&#xA;&lt;a class=&#34;header-anchor&#34; href=&#34;#why-language-models-are-important&#34;&gt;&lt;/a&gt;&#xD;&#xA;Why language models are important&#xD;&#xA;&lt;/h2&gt;&lt;blockquote&gt;&#xA;&lt;p&gt;[!PDF|yellow] [[Barektain et al_Foundational Large Language Models &amp;amp; Text Generation.pdf#page=7&amp;amp;selection=20,15,30,29&amp;amp;color=yellow|Barektain et al_Foundational Large Language Models &amp;amp; Text Generation, p.7]]&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Although foundational LLMs trained in a variety of tasks on large amounts of data perform very well out of the box and display emergent behaviors (e.g. the ability to perform tasks they have not been directly trained for) they can also be adapted to solve specific tasks where performance out of the box is not at the level desired through a process known as fine-tuning. This requires significantly less data and computational resources than training an LLM from scratch.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Placeholder Text</title>
      <link>//localhost:1313/post/placeholder-text/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/post/placeholder-text/</guid>
      <description>&lt;p&gt;Lorem est tota propiore conpellat pectoribus de pectora summo.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
